# Cassava-Leaf_Image-Classification

## Abstract 

For this project, our main goal was to be able to differentiate four different diseases on a cassava plant while also being able to recognize when the leaf is healthy (for a total of 5 possible classes). Being able to identify these diseases can lead to drastic improvements in the prevention of harmed food supply in several African countries. Our approach to this challenge has been to create several convolutional neural networks (CNNs) and fine-tune these models to result in an image classifier that can help identify which leaves are infected by each disease. Several models were created, tested, and evaluated for their resulting accuracies, including both pretrained ResNet models along with custom designed CNN architectures, deciding the number of layers, kernel size, stride, and max pool size on our own and investigating how the resulting model performs on the data. Preprocessing of the images with augmentations, balancing of the classes of diseases in the dataset, standardization/normalization of the pixels, and resizing was also performed, which were then fed into the designed models created for training, validation, and testing. From the several models attempted, our best model’s performance ended up having a test accuracy of 0.763, but this was before identifying an important detail about our data, being that it was skewed and not properly balanced (a majority of the samples belonged to a single class). After taking this into account and balancing the data appropriately, the model created with best performance ended up having a test accuracy of 0.55. 

## Introduction 

The dataset we worked with contained over 21,000 images of cassava plant leaves. As mentioned earlier, these leaves were either healthy or infected with one of four different diseases. This makes our dataset of images have 5 classes. Along with this dataset of images, there is a CSV file containing the image ID for each of these images along with a number label in the range of 0 to 4 to represent which of the 5 possible classes corresponds to each image. Labels 0 through 3 represent the different possible disease states for the leaves, Cassava Bacterial Blight (CBB), Cassava Brown Streak Disease (CBSD), Cassava Green Mottle (CGM), and Cassava Mosaic Disease (CMD), respectively. The label 4 represents a healthy cassava leaf. 

Once loaded with a data loader, the dataset was analyzed to make it sure it had a balanced mix of classes. The class with label 3 constituted over 60% of the labels, and thus we decided to balance the dataset by oversampling the rest of the classes using augmentation techniques. This method will both allow us to balance the classes in the data, and to augment the images in order to provide sufficient samples for the models to learn from. These images along with their corresponding label numbers, were now used to train, validate, and test our convolutional neural networks (CNNs). This resulted in CNNs that would take any image as an input and output the predicted class/label for the leaf image. 

We first used the original dataset's images and labels, augmented them using a mix of random flipping, rotation, cropping and normalization, all in order to both balance the classes in the data and increase the number of samples our models will train on. Normalization and standardization of the pixels were also performed, resulting in values ranging between -1 and 1. Once done, we split the data. For some models, the data was split into 80% training data and 20% testing data, and for others, a validation set was also included, therefore using 70% for training, 20% for validation, and 10% for testing. Next, multiple custom CNN architectures were created as well as pre-trained architectures like ResNet18. With choices of cross-entropy loss and Adam optimizer, we finally trained these models on the now augmented data.

We first evaluated our models' training using validation sets while training, and then used a set-aside test set for final analysis of overall performance of our models. We monitored both the accuracy and the correlation matrix of the testing, in order to make sure the model wasn't biased towards any specific classes.

## Background

For the creation of convolutional neural networks, we followed specific parameters that tend to be recognized as common optimal design choices for convolutional neural networks with good performance. For example, it is commonly recognized as good practice to resize such large images to have smaller dimensions. In addition, it is commonly well known that large kernel sizes and strides should be avoided. This is why kernel sizes used in some of our models never exceeded 4x4 and with strides no larger than 2x2. 
	
In addition, given that our dataset is quite large, we also wanted to experiment with deeper neural networks, which are expected to result in better performance for such large datasets. Given the recognized great performance of standard ResNet models, we also implemented a pre-trained ResNet18 model in order to analyze its performance compared to our much simpler custom-made models with fewer layers.

## Summary of Contributions 

On the implementation side, we will talk in detail about the augmentation transformations explored and used for preprocessing of the data, along with the oversampling and its use in balancing the classes and providing more data samples to train on. The different CNN architectures explored and used will also be discussed, including some custom-designed architectures for CNNs along with pre-trained CNN (ResNet) models. Finally, we will discuss the design decisions made for the training. On the evaluation side, we will mainly talk about the evaluation metrics we decided to focus on, such as validation accuracy, test accuracy, and a correlation matrix, and the reasons behind each. These were pivotal in assessing the overall performance of our models in order to determine which ones resulted in stronger outcomes and which specific changes to parameters and/or network architecture may also play a factor in improving the predictions of our models. 

### Implementation Contributions 

A variety of different techniques were attempted for all steps; several different preprocessing approaches were attempted, changes were done for some of the parameters of the models, and entirely different CNN models were created, trained, and tested as well to have a stronger understanding behind which models, parameters, and preprocessing steps seemed most beneficial for improved CNN models. 

In terms of preprocessing, a variety of augmentations were implemented. Some of the models had no augmentations implemented for the images while others included augmentations with the intent of generating more generalizable models for expected better performance. Some of these augmentations included random rotations and random flips of the images. One key preprocessing step shared among all models, however, was the normalization and standardization of the pixels for the images to be in the range between -1 and 1. Another common strategy used was changing the shape of the images. All images were 3 channel 600 x 800 images. For the training of convolutional neural networks, we wanted to reduce the size of these images. Different functions available on PyTorch were implemented for this, such as CenterCrop and ReSize, which allowed us to reduce the size of the images by a factor of 10, changing the shape of the images to be 60x80. These were examples of using both CenterCrop and ReSize, two different methods of changing the shape, in order to change to a specific new shape for the image. Random cropping was also performed in other models in order to also make our augmentations more randomized, again with the intent of making the model more generalizable. 

Before moving on to designing the architectures used for training, several mixes of the augmentations/transformations mentioned above were used for balancing the dataset. The original dataset came with a little over 21,000 images, with over 60% of them having the label "3". At first, we intentionally skipped balancing and went directly to modeling in order to compare the results of the two approaches later on. When training on data this unbalanced, we ended up scoring pretty high results in the validations and testing accuracies. These seemingly cheerful results were very misleading though, as the model was very biased towards guessing the label "3", and the unbalanced nature of our data rewarded this behavior. This was the main reason we decided to monitor  the correlation matrix in our evaluations as well. This unbalanced nature of the data was identified after we had already built, trained, and tested a few CNNs. All other CNNs created after this were trained on the corrected balanced data.

The way we balanced the data was fairly simple. We counted the occurrences of each label (class) in the data, and artificially augmented the data by randomly sampling images from minority classes, then applying the transformation mentioned above until they reached the frequency of the majority class ("3"). This catapulted our dataset from a mere 21k samples to over triple that amount. Of course, we could've forced this technique to then oversample everything equally in order to create an even larger dataset. However, we didn't want to saturate our samples with augmented data, as it doesn't exactly provide extra information to be learned by the models down the line. And including too much artificial data might even cause the models to develop unwanted biases.

The data was also split in order to properly have a manner of analyzing the performance of the models we created. We tried splitting the data in two different ways. First, we tried 70% of the data being for training, 20% for validation,a dn 10% for testing. We also did 80% training and 20% testing, without any validation due to limited time remaining in the project and very long runtimes for the training and use of these models. 

Multiple CNN models with different architectures and features were also created and tested in order to compare their performances. A custom three layer convolutional neural network was designed. The first layer changed from 3 channels to 9 channels, using a kernel size of 4x4 and stride of 1x1, followed by 2x2 max pooling with a 2x2 stride. The second layer was basically a copy of the first, with the only difference being the change in channels, from 9 channels to 6 channels. The third and last layer moved back to 3 channels, again, using a 4x4 kernel and 1x1 stride. A max pooling of the same size and stride was used again for this layer. ReLU was used as the activation function for each of these layers, and at the end of the convolutional neural network architecture, soft max probabilities are performed to generate the predictions. Adam was used as the optimizer for the neural network, using a learning rate of 1e-3. Cross entropy was used as the criterion during training for calculation of the loss and gradients in order to properly update the weights. This model resulted with training, validation, and test set accuracy scores that are about the same, with training accuracy of 0.644, validation accuracy of 0.649, and test accuracy of 0.608. 

We also explored deeper models consisting of 3 convolutional layers taking 3 channels as input and outputting 128 channels. Each of the first two were coupled with a 2D batch normalization layer as well as a maxpool with kernel_size=2. After all of this came two fully connected linear layers having a drop-out of 20% in order to avoid overfitting.

Next, we decided to follow prior works and use a pretrained ResNet model, a ResNet18 model specifically. This is an 18 layer neural network, which we expected could have better resulting performance given the depth of the neural network and the large size of the dataset. We would have liked to attempt other ResNet models that could likely have even better performance, due to having even more layers, such as ResNet34 (with 34 layers), ResNet50 (with 50 layers), and ResNet101 (with 101 layers). However, due to the long runtime for these neural networks, limited GPU usage (due to GPU usage limits on Google Colab), and the limited time available for work on the project, we did not get to try these models. The ResNet18 model was trained on the preprocessed data (including a CenterCrop of size 224x224), resulting in training accuracy of 0.771, validation accuracy of 0.769, and test accuracy of 0.763. This model therefore proved to have better performance than the custom-designed convolutional neural network with only three layers, which was not a surprising outcome (given the depth of the ResNet18 model). It is important to note that this model is one that was created and trained prior to realizing how the data was skewed and unbalanced. 

We weren't able to explore deeper networks or many different variations of the hyper-parameters, due to time limitations and mainly GPU usage limitations imposed by Google Colab. This not only stumped the exploration of possible architecture, but also the batch sizes and number of epochs we were able to train our models on. All models used were showing improvements throughout all of the epochs we were able to run them for. Thus showing that, if time and resources allowed, we could push them far past the performance we've seen so far.

## Evaluation Contributions 

When first starting to explore different models and monitoring their performance, the baseline we were comparing with was a random guesser. Given the balanced data, a model randomly guessing classes in a 5-class-multi-classification problem such as this one is expected to perform with 20% accuracy. But once we had our first few models fully trained and evaluated, we could mutually compare their performance. 

Before deciding to balance the data, as mentioned previously, a baseline model we tried performed with around 65% accuracy. This was misleading because of the model's obvious bias towards the majority class, which artificially inflated the testing results. This is what led us to choosing the correlation matrix as one of our main metrics for evaluation. Going forward, we heavily considered the correlation matrix when evaluating the models we tested, making sure that it wasn't biased towards any specific classes.

Once the data was balanced, our subsequent testing proved that the models were much less likely to favor any class in particular, and the correlation matrices showed unbiased behavior. Although the accuracy may have seemingly taken a dive, it was much more interesting still because of how the results represented the data in a truly unbiased and more realistic manner.

Most of the custom architectures we tried ranged between 55% and 65% accuracy (this is an even accuracy between all classes, without any bias). We also tried some state-of-the-art CNN models including a ResNet model. These of course performed better than our custom creations, hovering around 75% accuracy, but this was before balancing the data appropriately. If we would have had the time to retrain and test this ResNet model after balancing the data, it is expected that it would have had the best  performance, given that before the correction of the unbalanced data it already showed the best performance out of all the models we had trained and tested at the time. It also makes intuitive sense, since deeper neural networks (such as ResNet18, compared to our custom-made CNN architectures) tend to perform better on such large datasets.

We believe the performance we've seen from the models we've tested so far to be fairly good, especially considering the very low resources and time we had to train them. Given more epochs and a larger batch size, they were certain to improve, if not even hit the 90% accuracy benchmark.

## Compute/Other Resources Used

For this project, we mainly used Google Colab to create, train, and test our models, implementing the PyTorch library for access to deep learning and convolutional neural networks. Given that we used Google Colab, we had limitations on access to GPU, which happened to be a challenge while we were making progress in our project.

## Conclusions 

From this project we were able to obtain experience in the exploration of several different convolutional neural network architectures, their corresponding features and parameters, and how changes to these components can have a drastic effect on the performance of our models. We also recognized the importance of analyzing the data you are working with before using it to train any machine learning algorithms. This happened after recognizing how the dataset of images was unbalanced, being skewed with one particular class (out of five) having substantially more samples in the dataset. Although we were able to fix this for some of our models, other models we had previously created, trained, and tested did not get to use the corrected balanced data for training, leading to less realistic results/analysis of the model’s performance. For the models created, however, our best model’s performance ended up having a test accuracy of 0.763, but this was before identifying and correcting the unbalanced nature of the data. This was via the more complex and deeper neural network, ResNet18. After balancing the data appropriately, the model created with best performance ended up having a test accuracy of 0.55, which was one of the custom-designed CNNs. If we would have encountered this issue earlier along the timeline of completing this project, we likely would have had a more accurate model after the balancing of the data, given that the best performing model before this problem was identified and corrected was the ResNet18 model.

Given an opportunity to work further on this in the future, we would like to use a better working environment than Google Colab, in order to avoid the constant crashes during training and the GPU limitations forcing us to use a low number of epochs and batch size. AWS SageMaker would be a great alternative work environment if this work were to be continued in the near future. In addition, we would want to retrain and evaluate the created models without balancing of the data. If this were to be done, we believe it would result in a new CNN model marked as the best/most accurate model for the classification of images after balancing of the dataset. In addition, we would also like to add a new scoring metric for all models evaluated, F1 Score. This would function as an additional tool to help properly evaluate all created models and compare them to one another. 


  






